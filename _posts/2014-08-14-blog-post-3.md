---
title: 'Blog Post number 3'
date: 2024-04-11
permalink: /posts/2014/08/blog-post-3/
tags:
  - subword
  - LM
  - translation
---


======
I was checking out few repositories for Language translation and came across set of following keywords which got me more interested towards checking these out ...

 - mgizapp
 - moses
 - kenlm
 - sentencepiece
 - Boost
 - sacremoses
 
 
 
 
Moses and KenLM are two computer programs that help in translating text from one language to another. They work by analyzing large sets of parallel data, which are pairs of sentences in two different languages that mean the same thing. 

Moses uses a technique called Statistical Machine Translation (SMT) that relies on language-dependent rules, while KenLM is a language model that provides fast and efficient text processing. Both programs are often used together to achieve the best results.

Moses uses an estimation process to establish the correspondence between words and phrases in different languages. This process involves analyzing word co-occurrences and segments to identify the best translation. KenLM is a language model that helps Moses by predicting the probability of a sentence being valid in a particular language based on the words in the sentence.

In simple terms, Moses and KenLM are like a translator that can understand two languages and help you communicate between them. They can be used to translate text or speech from one language to another, making it easier to communicate with people who speak a different language.


Moses, a Statistical Machine Translation (SMT) system, relies on language-dependent rules, particularly for European languages. SMT translation systems are trained using large sets of parallel data. Parallel data comprises sentences in two different languages that are aligned, meaning each sentence in one language corresponds to its translated counterpart in the other language, forming what is commonly known as a bitext.

In Moses, the training process involves utilizing parallel data to establish translation correspondences between the two languages of interest. This is achieved by analyzing word cooccurrences and segments (referred to as phrases). In phrase-based machine translation, the correspondences are between continuous word sequences. In contrast, hierarchical phrase-based or syntax-based translation incorporates more structural elements into the correspondences.

**KenLM: Efficient Language Modeling**

KenLM stands out as a language model that offers both speed and low memory usage. It maintains compatibility with SRI probabilities, ensuring accuracy up to floating-point rounding. Developed by Ken Heafield, KenLM is bundled with Moses and is thread-safe for multi-threaded use, enhancing efficiency.

**Estimation Process**

The lmplz program estimates language models using Modified Kneser-Ney smoothing without pruning. Users specify parameters such as order (-o), memory allocation for building (-S), and temporary file location (-T). Notably, -S is compatible with GNU sort, where values like 1G represent gigabytes, and percentages denote RAM utilization. KenLM scales effectively to larger models compared to SRILM, avoiding approximation techniques like those used in IRSTLM.

For language-independent subword tokenization and detokenization, you can explore the SentencePiece model. This model offers efficient tokenization using subword units, facilitating tasks like training and processing multilingual text. Check out the following resources for further insights:

- [Moses Tutorial](http://www2.statmt.org/moses/?n=Moses.Tutorial)
- [Discriminative Reranking for Neural Machine Translation](https://aclanthology.org/2021.acl-long.563/)
